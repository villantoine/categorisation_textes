{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "categorisation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1HqGjgKFzFW54pPuXPJBGajSsDn5-OJ9m",
      "authorship_tag": "ABX9TyM9j6MwNs3AdNyv0Za7n9Ec",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villantoine/categorisation_textes/blob/main/categorisation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHAmoD1-zEB-"
      },
      "source": [
        "#####################################################################\n",
        "### ------ IMPORTS ET INSTALLATION DE LIBRAIRIES EXTERNES ------- ###\n",
        "#####################################################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x436WdRS2Kgk"
      },
      "source": [
        "# LIBRAIRIES EXTERNES \n",
        "\n",
        "!pip install mwparserfromhell #parser wikipedia\n",
        "!pip install pdfplumber #outil extraction pdf\n",
        "!python -m spacy download fr_core_news_sm #pipelines spacy, fr et en\n",
        "!python -m spacy download en_core_web_sm\n",
        "!pip install --user -U nltk #nltk\n",
        "\n",
        "# IL FAUT EXECUTER CETTE CELLULE ET RESTART LE RUNTIME POUR POUVOIR EXECUTER LE RESTE !!!! (Runtime > Restart runtime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDmayyNrepJP"
      },
      "source": [
        "# Création du dataset : exports et prétraitement\n",
        "\n",
        "import os\n",
        "import pdfplumber\n",
        "from textblob import TextBlob\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import spacy\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS\n",
        "import xml.sax\n",
        "import subprocess\n",
        "import re\n",
        "import mwparserfromhell\n",
        "\n",
        "# Entrainements, tests, affichage\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9IMvCf08zjmA"
      },
      "source": [
        "La cellule suivante est la fonction de nettoyage appliquée à tous les textes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m52bj1oxfd13"
      },
      "source": [
        "# Nettoyage (fonction commune)\n",
        "\n",
        "def cleaning(text_input):\n",
        "\n",
        "  tokenizer = nltk.tokenize.RegexpTokenizer(\"r'''\\w'|[A-zÀ-ú]+|[^\\w\\s]\")\n",
        "  nltk_stopword = nltk.corpus.stopwords.words('french') + nltk.corpus.stopwords.words('english')\n",
        "  global_stopwords = STOP_WORDS.copy()\n",
        "\n",
        "  b = TextBlob(text_input)\n",
        "  if (b.detect_language() == 'fr'):\n",
        "    nlp = spacy.load('fr_core_news_sm')\n",
        "  elif (b.detect_language() == 'en'):\n",
        "    nlp = spacy.load('en_core_web_sm')\n",
        "  else:\n",
        "    print(\"ni francais ni anglais / nor english nor french\")\n",
        "    raise ValueError\n",
        "\n",
        "  doc = nlp(text_input)\n",
        "  data_lemm = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "  data_tokenized = tokenizer.tokenize(data_lemm)\n",
        "  final_data = [word.lower() for word in data_tokenized if not word.lower() in global_stopwords and len(word) > 2]\n",
        "\n",
        "  return (\" \".join([word for word in final_data]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "er2gpYgszwGJ"
      },
      "source": [
        "Extraction des articles Wikipédia depuis un fichier XML (l.45)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ALPahFjmzLnA"
      },
      "source": [
        "# EXTRACTION DATA WIKI\n",
        "\n",
        "class WikiXmlHandler(xml.sax.handler.ContentHandler):\n",
        "    \"\"\"Content handler for Wiki XML data using SAX\"\"\"\n",
        "    def __init__(self):\n",
        "        xml.sax.handler.ContentHandler.__init__(self)\n",
        "        self._buffer = None\n",
        "        self._values = {}\n",
        "        self._current_tag = None\n",
        "        self._pages = []\n",
        "\n",
        "    def characters(self, content):\n",
        "        \"\"\"Characters between opening and closing tags\"\"\"\n",
        "        if self._current_tag:\n",
        "            self._buffer.append(content)\n",
        "\n",
        "    def startElement(self, name, attrs):\n",
        "        \"\"\"Opening tag of element\"\"\"\n",
        "        if name in ('title', 'text'):\n",
        "            self._current_tag = name\n",
        "            self._buffer = []\n",
        "\n",
        "    def endElement(self, name):\n",
        "        \"\"\"Closing tag of element\"\"\"\n",
        "        if name == self._current_tag:\n",
        "            self._values[name] = ' '.join(self._buffer)\n",
        "\n",
        "        if name == 'page':\n",
        "            self._pages.append((self._values['title'], self._values['text']))\n",
        "\n",
        "\n",
        "def clean_wiki(dirty_entry):\n",
        "  tmp = re.sub('==.*?==', '', dirty_entry)\n",
        "  tmp = re.sub('< ref >.*?< /ref > ', '', tmp)\n",
        "  tmp = re.sub('< gallery.*?< /gallery > ', '', tmp)\n",
        "  result = re.sub('[[Image.*?]]', '', tmp)\n",
        "\n",
        "  return result\n",
        "\n",
        "  \n",
        "\n",
        "handler = WikiXmlHandler()\n",
        "\n",
        "parser = xml.sax.make_parser()\n",
        "parser.setContentHandler(handler)\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/Stage - Caractérisation automatique de texte (D2IT)/Wikipédia-20210507095826.xml\", \"r\") #chemin d'un fichier xml contenant l'export d'une catégorie d'articles\n",
        "line = f.readline()\n",
        "while(line):\n",
        "      parser.feed(line)\n",
        "      line = f.readline()      \n",
        "f.close()\n",
        "\n",
        "## Cleaning\n",
        "\n",
        "texts = []\n",
        "\n",
        "for entry in handler._pages:\n",
        "  wiki = mwparserfromhell.parse(entry[1])\n",
        "  entry_text_stripped = wiki.strip_code(normalize=True)\n",
        "  entry_text_stripped = clean_wiki(entry_text_stripped) \n",
        "  texts.append(cleaning(entry_text_stripped))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAcVpCGS1I0K"
      },
      "source": [
        "Extraction des documents Stelia depuis des fichiers pdf (lignes 3 & 4) grâce à pdfplumber. Ils sont divisés en deux catégories selon leur contenu, très textuel ou non."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c60bXGJATcrZ"
      },
      "source": [
        "#EXTRACTION DOCS STELIA\n",
        "    \n",
        "chemin_docs_stelia = \"/content/drive/MyDrive/Stage - Caractérisation automatique de texte (D2IT)/docs_stelia_ok/\"\n",
        "chemin_docs_stelia_parfaits = \"/content/drive/MyDrive/Stage - Caractérisation automatique de texte (D2IT)/docs_stelia_parfaits/\"\n",
        "\n",
        "file_list = os.listdir(chemin_docs_stelia)\n",
        "file_list_parfait = os.listdir(chemin_docs_stelia_parfaits)\n",
        "\n",
        "pdf_data = []\n",
        "\n",
        "for i in file_list:\n",
        "  content = \"\"\n",
        "  with pdfplumber.open(chemin_docs_stelia + i) as pdf:\n",
        "    nb_pages = pdf.pages\n",
        "\n",
        "    for p in nb_pages:\n",
        "      content += \" \" + p.extract_text()\n",
        "\n",
        "  pdf_data.append(cleaning(content))\n",
        "\n",
        "for i in file_list_parfait:\n",
        "  content = \"\"\n",
        "  with pdfplumber.open(chemin_docs_stelia_parfaits + i) as pdf:\n",
        "    nb_pages = pdf.pages\n",
        "\n",
        "    for p in nb_pages:\n",
        "      content += \" \" + str(p.extract_text())\n",
        "\n",
        "  pdf_data.append(cleaning(content))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwezT2JO1iI6"
      },
      "source": [
        "Extraction des documents HAL/Scholar depuis des fichiers PDF (ligne 3) grâce à pdfplumber "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0q3BeZd5irj"
      },
      "source": [
        "# EXTRACTION HAL/SCHOLAR\n",
        "\n",
        "chemin_docs_HAL = \"/content/drive/MyDrive/Stage - Caractérisation automatique de texte (D2IT)/docs_HAL_aero_materiaux/\"\n",
        "\n",
        "file_list_HAL = os.listdir(chemin_docs_HAL)\n",
        "\n",
        "HAL_data = []\n",
        "\n",
        "for i in file_list_HAL:\n",
        "  content = \"\"\n",
        "  with pdfplumber.open(chemin_docs_HAL + i) as pdf:\n",
        "    nb_pages = pdf.pages\n",
        "\n",
        "    for p in nb_pages:\n",
        "      content += \" \" + str(p.extract_text())\n",
        "\n",
        "  HAL_data.append(cleaning(content))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0dHqszD1rC6"
      },
      "source": [
        "Fonction d'affichage du résultat des entraînement sous forme de matrice avec en abscisse le label réel, en ordonnée le label prédit par le modèle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1BvQKLQ_wSwS"
      },
      "source": [
        "# affichage (fonction commune)\n",
        "\n",
        "def display(y_true,y_model):\n",
        "\n",
        "  mat = confusion_matrix(y_true, y_model)\n",
        "  sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
        "            xticklabels=[\"HAL\",\"stelia\",\"wiki\"], yticklabels=[\"HAL\",\"stelia\",\"wiki\"])\n",
        "  plt.xlabel('true label')\n",
        "  plt.ylabel('predicted label')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzdFCKVQ13wN"
      },
      "source": [
        "Les deux cellules suivantes finalisent la constitution du dataset. La première est spécifique à ce problème, la deuxième est plus globale."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqmuL3Vp1IQK"
      },
      "source": [
        "# DATASET CREATION\n",
        "\n",
        "X = texts + pdf_data + HAL_data\n",
        "y = ['wiki'] * len(texts) + ['stelia'] * len(pdf_data) + ['HAL'] * len(HAL_data)\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-YTHbPzZ1hZ"
      },
      "source": [
        "# DATA SPLITTING\n",
        "\n",
        "def data_splitting(X,y):\n",
        "  return train_test_split(X, y, test_size=0.25)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycOSs9wG2Eqv"
      },
      "source": [
        "Les cinq cellules suivantes sont l'implémentation des différents modèles avec à chaque fois : \n",
        "\n",
        "* Leur import  \n",
        "* La création d'un pipeline avec Tfidf\n",
        "* Leur entraînement\n",
        "* Le test et l'affichage des résultats\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOxZ98vigO7L"
      },
      "source": [
        "# NAIVE BAYES\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "model = make_pipeline(TfidfVectorizer(), MultinomialNB(0.1,False))\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_multinb = model.predict(X_test)\n",
        "\n",
        "display(y_test,y_multinb)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1SN0kh4Y_xZ"
      },
      "source": [
        "# LOGISTIC REGRESSION\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "Multiclass_model = make_pipeline(TfidfVectorizer(),LogisticRegression(multi_class='ovr'))\n",
        "Multiclass_model.fit(X_train, y_train)\n",
        "\n",
        "y_logreg = Multiclass_model.predict(X_test)\n",
        "\n",
        "display(y_test,y_logreg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQEHoz_lZCWc"
      },
      "source": [
        "# DECISION TREE\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = make_pipeline(TfidfVectorizer(), RandomForestClassifier(n_estimators=1000,criterion=\"gini\"))\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_tree = model.predict(X_test)\n",
        "\n",
        "display(y_test,y_tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW-SbDF_ZITJ"
      },
      "source": [
        "# KNN\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model = make_pipeline(TfidfVectorizer(), KNeighborsClassifier(n_neighbors=18))\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_knn = model.predict(X_test)\n",
        "\n",
        "display(y_test,y_knn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XofMncsKGRI"
      },
      "source": [
        "# SVC\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model = make_pipeline(TfidfVectorizer(), SVC(C = 10, degree = 2, probability = True, tol = 1))\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_svc = model.predict(X_test)\n",
        "\n",
        "display(y_test,y_svc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_-W-lzV2X6N"
      },
      "source": [
        "Cellule dédiée au test des modèles. Ils sont tous les 5 entraînés 100 fois sur un échantillon de données différent et leur précision est affichée à la fin."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr92rDwmb_wW"
      },
      "source": [
        "# TESTING ALL THE MODELS (execution time, accuracy)\n",
        "\n",
        "from time import time\n",
        "\n",
        "def train_model(input,X_train,y_train):\n",
        "  model = make_pipeline(TfidfVectorizer(), input)\n",
        "  model.fit(X_train, y_train)\n",
        "  return model\n",
        "\n",
        "model_labels = [\"multinb\",\"logreg\",\"tree\",\"knn\",\"svc\"]\n",
        "models = [MultinomialNB(0.1,False),LogisticRegression(multi_class='ovr'),RandomForestClassifier(n_estimators=1000,criterion=\"gini\"),KNeighborsClassifier(n_neighbors=18),SVC(C = 10, degree = 2, probability = True, tol = 1)]\n",
        "acc_models = [0,0,0,0,0]\n",
        "n = 100\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  print(\"Evaluating \" + model_labels[i] + \"...\")\n",
        "  t1 = time()\n",
        "  \n",
        "  for k in range(n):\n",
        "    X_train, X_test, y_train, y_test = data_splitting(X,y)\n",
        "    trained_model = train_model(models[i],X_train,y_train)\n",
        "    acc_models[i] += trained_model.score(X_test,y_test) \n",
        "  acc_models[i] /= n\n",
        "  \n",
        "  t2 = time()\n",
        "  print(\"Temps écoulé : \" + str(t2-t1))\n",
        "\n",
        "print(acc_models)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "61o5tLuM-aM0"
      },
      "source": [
        "######################################\n",
        "### ------ PARAMETRISATION ------- ###\n",
        "######################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COl_FWz62msV"
      },
      "source": [
        "Les cinq cellules suivantes sont la paramétrisation des différents modèles avec à chaque fois :\n",
        "\n",
        "* La définition d'une grille de paramètre propre au modèle\n",
        "* La création du pipeline habituel en remplaçant le modèle par un GridSearchCV\n",
        "* L'entraînement et l'affichage des résultats (la meilleure combinaison de paramètres)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfNCQY2oKO1T"
      },
      "source": [
        "# NAIVE BAYES - PARAMETRISATION\n",
        "\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "param_grid = {'alpha': [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1, 2],  \n",
        "              'fit_prior': [True,False],\n",
        "              }  \n",
        "   \n",
        "grid = make_pipeline(TfidfVectorizer(),GridSearchCV(MultinomialNB(), param_grid, refit = True, verbose = 3,n_jobs=-1)) \n",
        "   \n",
        "# fitting the model for grid search \n",
        "grid.fit(X_train, y_train) \n",
        " \n",
        "# print best parameter after tuning \n",
        "print(grid[1].best_params_) \n",
        "grid_predictions = grid.predict(X_test) \n",
        "   \n",
        "# print classification report \n",
        "print(classification_report(y_test, grid_predictions)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdF5G7V7b6nX"
      },
      "source": [
        "# LOGISTIC REGRESSION - PARAMETRISATION\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "param_grid = {'penalty': [\"l1\",\"l2\",\"elasticnet\",\"none\"],  \n",
        "              'tol': [1,1e-1,1e-2,1e-3,1e-4],\n",
        "              'C': [0.01, 0.1, 1, 10, 100, 1000],\n",
        "              'solver' : [\"newton-cg\",\"lbfgs\",\"liblinear\",\"sag\",\"saga\"]\n",
        "              }  \n",
        "   \n",
        "grid = make_pipeline(TfidfVectorizer(),GridSearchCV(LogisticRegression(), param_grid, refit = True, verbose = 3,n_jobs=-1)) \n",
        "   \n",
        "# fitting the model for grid search \n",
        "grid.fit(X_train, y_train) \n",
        " \n",
        "# print best parameter after tuning \n",
        "print(grid[1].best_params_) \n",
        "grid_predictions = grid.predict(X_test) \n",
        "   \n",
        "# print classification report \n",
        "print(classification_report(y_test, grid_predictions)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5C7aDDzyc4dF"
      },
      "source": [
        "# DECISION TREE - PARAMETRISATION\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "param_grid = {'n_estimators': [10,100,1000,10000],  \n",
        "              'criterion': [\"gini\",\"entropy\"]\n",
        "              }  \n",
        "   \n",
        "grid = make_pipeline(TfidfVectorizer(),GridSearchCV(RandomForestClassifier(), param_grid, refit = True, verbose = 3,n_jobs=-1)) \n",
        "   \n",
        "# fitting the model for grid search \n",
        "grid.fit(X_train, y_train) \n",
        " \n",
        "# print best parameter after tuning \n",
        "print(grid[1].best_params_) \n",
        "grid_predictions = grid.predict(X_test) \n",
        "   \n",
        "# print classification report \n",
        "print(classification_report(y_test, grid_predictions)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUhPEfeMd0T0"
      },
      "source": [
        "# KNN - PARAMETRISATION\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "param_grid = {'n_neighbors': [15,16,17,18,19,20]\n",
        "              }  \n",
        "   \n",
        "grid = make_pipeline(TfidfVectorizer(),GridSearchCV(KNeighborsClassifier(), param_grid, refit = True, verbose = 3,n_jobs=-1)) \n",
        "   \n",
        "# fitting the model for grid search \n",
        "grid.fit(X_train, y_train) \n",
        " \n",
        "# print best parameter after tuning \n",
        "print(grid[1].best_params_) \n",
        "grid_predictions = grid.predict(X_test) \n",
        "   \n",
        "# print classification report \n",
        "print(classification_report(y_test, grid_predictions)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "BANF9RuJ2gCP"
      },
      "source": [
        "# SVC - PARAMETRISATION\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "param_grid = {'C': [0.1, 1, 10, 100],\n",
        "              'kernel' : [\"linear\",\"poly\",\"rbf\",\"sigmoid\"],\n",
        "              'degree' : [2,3,4],\n",
        "              'shrinking' : [True,False],\n",
        "              'probability' : [True,False],\n",
        "              'tol': [1,1e-1,1e-2,1e-3,1e-4],\n",
        "              }  \n",
        "   \n",
        "grid = make_pipeline(TfidfVectorizer(),GridSearchCV(SVC(), param_grid, refit = True, verbose = 3,n_jobs=-1)) \n",
        "   \n",
        "# fitting the model for grid search \n",
        "grid.fit(X_train, y_train) \n",
        " \n",
        "# print best parameter after tuning \n",
        "print(grid[1].best_params_) \n",
        "grid_predictions = grid.predict(X_test) \n",
        "   \n",
        "# print classification report \n",
        "print(classification_report(y_test, grid_predictions)) \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4OuV_eh27qI"
      },
      "source": [
        "Dernière partie du projet, début d'entraînement sur le contenu des documents Stelia grâce à un document texte précisant le thème de chacun (ligne 11). Le reste est un entraînement classique et un test sur les thèses de HAL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9lE1QmagI59"
      },
      "source": [
        "######################################################################\n",
        "###  ENTRAINEMENT SUR LE THEME DES DOCS STELIA AVEC DECISION TREE  ###\n",
        "######################################################################\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "X = pdf_data #documents Stelia, voir plus haut\n",
        "y = []\n",
        "\n",
        "labels_table = []\n",
        "labels_doc = open(\"/content/drive/MyDrive/Stage - Caractérisation automatique de texte (D2IT)/labels_stelia.txt\", \"r\")\n",
        "line = labels_doc.readline()\n",
        "while(line):\n",
        "  line1 = line\n",
        "  line = labels_doc.readline()\n",
        "  labels_table.append([line1,line])\n",
        "  line = labels_doc.readline()\n",
        "labels_doc.close()\n",
        "\n",
        "for i in range(len(labels_table)):\n",
        "  y.append(labels_table[i][1])  \n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X, y, test_size=0.25)\n",
        "\n",
        "model = make_pipeline(TfidfVectorizer(), RandomForestClassifier(n_estimators=1000,criterion=\"gini\"))\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_tree = model.predict(X_test)\n",
        "\n",
        "#display(y_test,y_tree)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IE23CmCAYYtB"
      },
      "source": [
        "chemin_docs_HAL = \"/content/drive/MyDrive/Stage - Caractérisation automatique de texte (D2IT)/docs_HAL_aero_materiaux/\"\n",
        "\n",
        "file_list_HAL = os.listdir(chemin_docs_HAL)\n",
        "\n",
        "for i in range(len(HAL_data)):\n",
        "  print(file_list_HAL[i])\n",
        "  pred = model.predict([HAL_data[i]])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}