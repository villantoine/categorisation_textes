{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "comptage_framatome.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1C_5aXplx4gEaTWTrn6_dCKCcApbm8Urr",
      "authorship_tag": "ABX9TyP0IHOfqJ5w6iIDpQX3sn7i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/villantoine/categorisation_textes/blob/main/comptage_framatome.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZwY5axs8xJZ"
      },
      "source": [
        "!python -m spacy download fr_core_news_sm\n",
        "!pip install --user -U nltk\n",
        "\n",
        "#RESTART THE RUNTIME AFTER THIS !!!! (Runtime > Restart runtime)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4rq0VKW3Kwr"
      },
      "source": [
        "import spacy\n",
        "import nltk\n",
        "from nltk.util import ngrams\n",
        "import os\n",
        "from time import time\n",
        "from spacy.lang.fr.stop_words import STOP_WORDS\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "##COMPTAGE MOTS - NETTOYAGE AVEC SPACY\n",
        "\n",
        "directory_path = \"/content/drive/MyDrive/comptage_framatome/\"\n",
        "file_list = os.listdir(directory_path + \"champs_texte_bruts/\")\n",
        "\n",
        "file_amount = len(file_list)\n",
        "files_treated = 0\n",
        "\n",
        "nlp = spacy.load('fr_core_news_sm')\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer(\"r'''\\w'|[A-zÀ-ú]+|[^\\w\\s]\")\n",
        "\n",
        "# STOP-WORDS\n",
        "\n",
        "user_list = open(directory_path + \"user_words.txt\",\"r\")\n",
        "data_user = user_list.read()\n",
        "data_user = data_user.split()\n",
        "user_list.close()\n",
        "\n",
        "nltk_stopword = nltk.corpus.stopwords.words('french') + nltk.corpus.stopwords.words('english')\n",
        "\n",
        "global_stopwords = STOP_WORDS.copy()\n",
        "\n",
        "for i in data_user:\n",
        "  global_stopwords.add(i)\n",
        "\n",
        "for i in nltk_stopword:\n",
        "  global_stopwords.add(i) \n",
        "\n",
        "t0 = time()\n",
        "\n",
        "# CHARGEMENT TEXTE\n",
        "\n",
        "for f in file_list:\n",
        "  file_name = directory_path + \"champs_texte_bruts/\" + f  \n",
        "  \n",
        "  text = open(file_name,\"r\",encoding='latin-1')\n",
        "  data = text.read()\n",
        "  text.close()\n",
        "\n",
        "# LEMMATIZATION\n",
        "\n",
        "  doc = nlp(data)\n",
        "  data_lemm = \" \".join([token.lemma_ for token in doc])\n",
        "\n",
        "# NETTOYAGE\n",
        "\n",
        "  data_tokenized = tokenizer.tokenize(data_lemm)\n",
        "  final_data = [word.lower() for word in data_tokenized if not word.lower() in global_stopwords and len(word) > 2]\n",
        "\n",
        "# BIGRAMS & TRIGRAMS\n",
        "\n",
        "  bigrams = ngrams(final_data,2)\n",
        "  bigrams_result = [ '_'.join(grams) for grams in bigrams]\n",
        "  trigrams = ngrams(final_data,3)\n",
        "  trigrams_result = [ '_'.join(grams) for grams in trigrams]\n",
        "\n",
        "# STOCKAGE RESULTATS\n",
        "\n",
        "  result_spacy = open(\"/content/drive/MyDrive/comptage_framatome/results_spacy/results_\" + f,\"w\")\n",
        "  result_spacy.write(\" \".join([word for word in final_data]))\n",
        "  result_spacy.close()\n",
        "\n",
        "  result_spacy_ngrams = open(\"/content/drive/MyDrive/comptage_framatome/results_spacy/ngram_\" + f,\"w\",encoding='latin-1')\n",
        "  for m in bigrams_result:\n",
        "    result_spacy_ngrams.write(m + \" \")\n",
        "  for m in trigrams_result:\n",
        "    result_spacy_ngrams.write(m + \" \")\n",
        "  result_spacy_ngrams.close()\n",
        "\n",
        "  time_elapsed = time() - t0\n",
        "  files_treated += 1\n",
        "  print(\"[\" + str(files_treated) + \"/\" + str(file_amount) + \"] \" + f + \" traité. Temps écoulé : \" + str(round(time_elapsed)) + \"s\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HToANRYQg1nx"
      },
      "source": [
        "##COMPTAGE MOTS FRAMATOME - INITIALISATION - ANCIENNE VERSION\n",
        "\n",
        "import os\n",
        "from nltk.util import ngrams\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "path_dossier = \"/content/drive/MyDrive/comptage_framatome/\"\n",
        "file_list = os.listdir(path_dossier + \"champs_texte_bruts/\")\n",
        "\n",
        "stopword_ext = open(\"/content/drive/MyDrive/comptage_framatome/stop_words_french.txt\")\n",
        "data_stopword = stopword_ext.read()\n",
        "stopword_ext.close()\n",
        "\n",
        "user_list = open(path_dossier + \"user_words.txt\",\"r\")\n",
        "data_user = user_list.read()\n",
        "user_list.close()\n",
        "\n",
        "stopword_list = nltk.corpus.stopwords.words('french') + nltk.corpus.stopwords.words('english') + ['...', \"lors\"] + data_stopword.split() + data_user.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vtJfm6yFiq6"
      },
      "source": [
        "#COMPTAGE DE MOTS FRAMATOME - NETTOYAGE ANCIENNE VERSION\n",
        "\n",
        "tokenizer = nltk.tokenize.RegexpTokenizer(\"r'''\\w'|[A-zÀ-ú]+|[^\\w\\s]\")\n",
        "\n",
        "for f in file_list:\n",
        "  file_name = path_dossier + \"champs_texte_bruts/\" + f  \n",
        "  \n",
        "  file = open(file_name,\"r\",encoding='latin-1')\n",
        "  data = file.read()\n",
        "  file.close()\n",
        "  \n",
        "  data_tokens_regexp = tokenizer.tokenize(data)\n",
        "  final_data = [word.lower() for word in data_tokens_regexp if not word.lower() in stopword_list and word.lower() and len(word) > 2]\n",
        "\n",
        "  # bigrams = ngrams(final_data,2)\n",
        "  # bigrams_result = [ '_'.join(grams) for grams in bigrams]\n",
        "  # trigrams = ngrams(final_data,3)\n",
        "  # trigrams_result = [ '_'.join(grams) for grams in trigrams]\n",
        "\n",
        "  ## résultat sous forme de liste de mot - directement sur le Drive\n",
        "\n",
        "  result_file_text = open(\"/content/drive/MyDrive/comptage_framatome/results_text/monogram\" + f,\"w\",encoding='latin-1')\n",
        "  for m in final_data:\n",
        "    result_file_text.write(m + \" \")\n",
        "  result_file_text.close()\n",
        "  \n",
        "  # result_file_text = open(\"/content/drive/MyDrive/comptage_framatome/results_text/ngram\" + f,\"w\",encoding='latin-1')\n",
        "  # for m in bigrams_result:\n",
        "  #   result_file_text.write(m + \" \")\n",
        "  # for m in trigrams_result:\n",
        "  #   result_file_text.write(m + \" \")\n",
        "  # result_file_text.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}